# Boehringer Ingelheim â€” HCP Web Crawler AI Agent

> **AI-powered Healthcare Provider contact discovery from the open internet.**
> Upload an Excel sheet of HCP records â†’ the AI agent autonomously searches, extracts, verifies, and records contact details with source URLs for compliance.

---

## âœ¨ Key Features

| Feature | Description |
|---|---|
| **Agentic Search** | LangGraph-orchestrated pipeline with multi-tier query strategy and conditional retry |
| **Stealth Browsing** | PyDoll headless Chrome with human-like typing, evasion, and resource blocking |
| **LLM Extraction** | GPT-4o-mini powered contact extraction + identity verification with confidence scoring |
| **Compliance** | Source URLs recorded for every data point |
| **Real-time Monitor** | Live progress tracking via Streamlit dashboard |
| **Enterprise Ready** | PostgreSQL, Docker, Helm charts, Jenkins CI/CD, OpenShift deployment |

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph Frontend["ğŸ–¥ï¸ Streamlit Dashboard"]
        UI["Upload / Monitor / Results"]
    end

    subgraph Backend["âš™ï¸ FastAPI Backend"]
        API["REST API"]
        DB["PostgreSQL"]
    end

    subgraph Agent["ğŸ§  LangGraph Agent Pipeline"]
        QRY["Query Builder"]
        SRCH["Google Search<br/>(PyDoll)"]
        SCRP["Page Scraper<br/>(PyDoll)"]
        LLM_E["LLM Extract<br/>(GPT-4o-mini)"]
        LLM_V["LLM Verify<br/>(GPT-4o-mini)"]
    end

    UI -->|"Upload .xlsx"| API
    API -->|"Create Job"| DB
    API -->|"Invoke per HCP"| QRY
    QRY --> SRCH
    SRCH --> SCRP
    SCRP --> LLM_E
    LLM_E --> LLM_V
    LLM_V -->|"Save Results"| DB
    DB -->|"Poll Status"| UI

    style Frontend fill:#08312A,color:#fff
    style Backend fill:#0d4a3e,color:#fff
    style Agent fill:#1a6b55,color:#fff
```

---

## ğŸ”„ Agent Workflow

Each HCP record flows through a **6-node LangGraph state machine** with conditional retry across 3 query tiers:

```mermaid
stateDiagram-v2
    [*] --> BuildQueries
    BuildQueries --> GoogleSearch
    GoogleSearch --> ScrapePages
    ScrapePages --> LLMExtract
    LLMExtract --> LLMVerify

    LLMVerify --> PrepareRetry: confidence < threshold\n& more tiers available
    LLMVerify --> Done: FOUND or\nall tiers exhausted

    PrepareRetry --> GoogleSearch: next query tier

    Done --> [*]

    note right of BuildQueries
        Tier 1: site:doximity.com OR site:npiprofile.com
        Tier 2: site:.gov OR site:.edu
        Tier 3: General search (no site filter)
    end note
```

### What Happens at Each Node

| Node | Technology | What It Does |
|---|---|---|
| **BuildQueries** | Python | Constructs 3 tiered Google queries from HCP name + location |
| **GoogleSearch** | PyDoll (Chrome CDP) | Opens headless Chrome, types query, parses result links |
| **ScrapePages** | PyDoll (Chrome CDP) | Visits top 5 URLs, blocks images/CSS, extracts raw text |
| **LLMExtract** | Azure OpenAI / OpenAI | Extracts phone, email, address from raw page text |
| **LLMVerify** | Azure OpenAI / OpenAI | Verifies identity match â†’ confidence score (0-100) |
| **PrepareRetry** | Python | Advances to next query tier if confidence < threshold |

---

## ğŸ§° Technology Stack

```mermaid
graph LR
    subgraph Orchestration
        LG["LangGraph"]
    end
    subgraph Browser
        PD["PyDoll<br/>(Chrome CDP)"]
    end
    subgraph LLM
        OAI["Azure OpenAI<br/>/ OpenAI"]
    end
    subgraph Backend
        FA["FastAPI"]
        SA["SQLAlchemy"]
        PG["PostgreSQL"]
    end
    subgraph Frontend
        ST["Streamlit"]
    end
    subgraph Infra
        DK["Docker"]
        HM["Helm"]
        JK["Jenkins"]
        OS["OpenShift"]
    end

    LG --> PD
    LG --> OAI
    FA --> SA --> PG
    FA --> LG
    ST --> FA

    style Orchestration fill:#08312A,color:#fff
    style Browser fill:#0d4a3e,color:#fff
    style LLM fill:#1a6b55,color:#fff
```

| Layer | Choice | Why |
|---|---|---|
| Agent Orchestration | **LangGraph** | Production-grade stateful graphs, checkpointing, conditional branching |
| Browser Automation | **PyDoll** | CDP-native, no WebDriver, async, evasion-first |
| LLM | **Azure OpenAI / OpenAI** | Dual provider support, GPT-4o-mini for cost-efficiency |
| Backend | **FastAPI** | Async-native, Pydantic integration, auto-docs |
| Database | **PostgreSQL** | Production-grade, async via `asyncpg` |
| Frontend | **Streamlit** | Rapid dashboard with custom CSS |
| Package Manager | **UV** | 10-100x faster than pip |
| Deploy | **Docker + Helm + Jenkins** | Enterprise CI/CD to OpenShift |

---

## ğŸš€ Quick Start (Single Command)

```bash
# Clone
git clone https://github.com/bokey007/HCP_web_crawller.git
cd HCP_web_crawller

# Run everything
./start.sh
```

The startup script handles all 6 steps automatically:

| Step | What It Does |
|---|---|
| 1 | Kills any existing API/UI processes |
| 2 | Checks for `uv`, installs if missing, creates virtualenv |
| 3 | Starts PostgreSQL via `docker compose up -d` |
| 4 | Creates `.env` from `.env.example` if missing |
| 5 | Generates sample Excel with real HCP records |
| 6 | Starts FastAPI (port 8000) + Streamlit (port 8501) |

> **Dashboard:** http://localhost:8501  
> **API Docs:** http://localhost:8000/docs  
> **Health:** http://localhost:8000/api/v1/health

Press `Ctrl+C` to cleanly shut down all services.

---

## ğŸ› ï¸ Manual Setup (Step-by-Step)

### Prerequisites

- Python 3.12+
- Docker (for PostgreSQL)
- Google Chrome / Chromium (for PyDoll)
- An OpenAI or Azure OpenAI API key

### 1. Install Dependencies

```bash
pip install uv
uv sync --all-extras
```

### 2. Start PostgreSQL

```bash
docker compose up -d
```

### 3. Configure Environment

```bash
cp .env.example .env
```

Edit `.env` and set your LLM credentials:

```dotenv
# Choose provider
LLM_PROVIDER=openai              # or "azure_openai"

# OpenAI
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini

# Azure OpenAI (if using azure_openai)
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-key
AZURE_OPENAI_DEPLOYMENT=gpt-4o-mini
```

### 4. Generate Sample Data

```bash
uv run python sample_data/create_sample.py
```

### 5. Start Backend

```bash
uv run uvicorn hcp_crawler.main:app --host 0.0.0.0 --port 8000 --reload
```

### 6. Start Frontend

```bash
uv run streamlit run frontend/app.py --server.port 8501 --server.headless true
```

### 7. Run Tests

```bash
uv run pytest tests/ -v
```

---

## ğŸ“ Project Structure

```
HCP_web_crawller/
â”œâ”€â”€ src/hcp_crawler/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app with lifespan events
â”‚   â”œâ”€â”€ config.py                  # pydantic-settings (env-driven config)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py              # REST endpoints (upload, jobs, results, export, stats)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ schemas.py             # Pydantic I/O models
â”‚   â”‚   â””â”€â”€ database.py            # SQLAlchemy ORM (ProcessingJob, HCPRecord)
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ session.py             # Async engine + session factory
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ excel_service.py       # Excel parsing (openpyxl)
â”‚   â”‚   â”œâ”€â”€ search_service.py      # Query builder + URL ranking
â”‚   â”‚   â”œâ”€â”€ scraper_service.py     # PyDoll browser pool + Google parser
â”‚   â”‚   â”œâ”€â”€ llm_service.py         # LLM extraction + verification
â”‚   â”‚   â”œâ”€â”€ stats_service.py       # Impact metrics calculator
â”‚   â”‚   â””â”€â”€ agent/
â”‚   â”‚       â”œâ”€â”€ state.py           # LangGraph TypedDict state schema
â”‚   â”‚       â”œâ”€â”€ nodes.py           # 6 async pipeline nodes
â”‚   â”‚       â””â”€â”€ graph.py           # StateGraph definition + compilation
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py              # Structured logging (structlog)
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                     # Streamlit dashboard (BI branded)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_excel_service.py      # 6 tests
â”‚   â”œâ”€â”€ test_search_service.py     # 9 tests
â”‚   â”œâ”€â”€ test_llm_service.py        # 6 tests
â”‚   â””â”€â”€ test_api.py                # 4 tests (+ async DB fixtures)
â”œâ”€â”€ helm/                          # Kubernetes / OpenShift deployment
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ deployment.yaml        # API + UI sidecar pods
â”‚       â”œâ”€â”€ service.yaml           # ClusterIP service
â”‚       â”œâ”€â”€ configmap.yaml         # Non-sensitive config
â”‚       â””â”€â”€ ingress.yaml           # OpenShift Routes (TLS)
â”œâ”€â”€ docker-compose.yml             # Local PostgreSQL
â”œâ”€â”€ Dockerfile                     # Multi-stage (UV + Chromium)
â”œâ”€â”€ Jenkinsfile                    # CI/CD pipeline
â”œâ”€â”€ start.sh                       # One-command launcher
â”œâ”€â”€ pyproject.toml                 # UV/pip dependencies
â””â”€â”€ .env.example                   # Environment template
```

---

## ğŸ”§ Configuration Reference

All settings are loaded from `.env` via `pydantic-settings`:

| Variable | Default | Description |
|---|---|---|
| `LLM_PROVIDER` | `azure_openai` | `azure_openai` or `openai` |
| `OPENAI_API_KEY` | â€” | OpenAI API key |
| `OPENAI_MODEL` | `gpt-4o-mini` | Model to use |
| `AZURE_OPENAI_ENDPOINT` | â€” | Azure endpoint URL |
| `AZURE_OPENAI_API_KEY` | â€” | Azure API key |
| `AZURE_OPENAI_DEPLOYMENT` | `gpt-4o-mini` | Azure deployment name |
| `DATABASE_URL` | `postgresql+asyncpg://...` | Database connection string |
| `MAX_CONCURRENT_BROWSERS` | `3` | Simultaneous PyDoll instances |
| `MAX_RESULTS_PER_HCP` | `5` | Google results to check per query |
| `CONFIDENCE_THRESHOLD` | `70` | Min score to mark as FOUND (0-100) |
| `SEARCH_TIMEOUT_SECONDS` | `30` | Google search timeout |
| `PAGE_LOAD_TIMEOUT_SECONDS` | `15` | Page scraping timeout |
| `MANUAL_MINUTES_PER_RECORD` | `15` | For "hours saved" metric |
| `HOURLY_RATE_USD` | `50` | For "dollars saved" metric |

---

## ğŸ³ Production Deployment

### Docker

```bash
docker build -t hcp-web-crawler:latest .
docker run -p 8000:8000 -p 8501:8501 --env-file .env hcp-web-crawler:latest
```

### Helm (OpenShift / Kubernetes)

```bash
# Create secret with API keys
kubectl create secret generic hcp-crawler-secrets \
  --from-literal=OPENAI_API_KEY=sk-proj-... \
  --from-literal=DATABASE_URL=postgresql+asyncpg://...

# Deploy
helm install hcp-crawler ./helm \
  --set image.repository=your-registry/hcp-web-crawler \
  --set image.tag=latest
```

### Jenkins CI/CD

The included `Jenkinsfile` automates:
1. **Lint** â€” `ruff check`
2. **Test** â€” `pytest` with coverage
3. **Build** â€” Docker image
4. **Push** â€” To container registry
5. **Deploy** â€” Helm upgrade to OpenShift

---

## ğŸ“Š Dashboard KPIs

The Streamlit dashboard tracks these impact metrics in real-time:

| Metric | Formula |
|---|---|
| **Records Processed** | Total HCPs processed across all jobs |
| **HCPs Found** | Records with confidence â‰¥ threshold |
| **Success Rate** | Found / Total Ã— 100% |
| **Hours Saved** | Records Ã— 15 min Ã· 60 |
| **Dollars Saved** | Hours Saved Ã— $50/hr |

---

## ğŸ”’ Compliance & Data Sources

The agent prioritizes trusted, publicly accessible sources:

| Priority | Source | Examples |
|---|---|---|
| ğŸ¥‡ Tier 1 | Medical directories | doximity.com, npiprofile.com |
| ğŸ¥ˆ Tier 2 | Government / Education | .gov, .edu domains |
| ğŸ¥‰ Tier 3 | General web | Hospital sites, health directories |
| ğŸš« Blocked | Social media | Facebook, Twitter, LinkedIn, Instagram |

Every extracted data point includes the source URL for full audit traceability.

---

## ğŸ“„ License

Internal use â€” Boehringer Ingelheim.
